{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb99e45-1ec1-47a9-807a-5acdef5911bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: findspark\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "Successfully installed findspark-2.0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lflow (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ipp (/opt/miniconda/envs/algos/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d45979-a1ae-4227-a2b4-bb04027e1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rahel/spark-bid-algo\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys, os\n",
    "\n",
    "# importing path\n",
    "athena_path = f\"{os.environ['HOME']}/spark-bid-algo\"\n",
    "# !cd $cpe_path ; git pull \n",
    "\n",
    "print(athena_path)\n",
    "if not athena_path in sys.path:\n",
    "    sys.path.append(athena_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4763076d-699a-4edb-b474-ad91c666144e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m createSparkConnection, closeSparkConnection\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mathena_connection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AthenaConnection\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscore_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Score_T\n",
      "File \u001b[0;32m~/spark-bid-algo/spark/connection.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# spark imports\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m      6\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "from spark.connection import createSparkConnection, closeSparkConnection\n",
    "from athena_connection import AthenaConnection\n",
    "\n",
    "from components.transformers.score_transformer import Score_T\n",
    "from components.transformers.parameters.score_parameters import *\n",
    "\n",
    "from model.score import adlogdata\n",
    "from model.config import adconfig\n",
    "from model.bid import admodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1906f20-7a9f-4328-9cfa-4dffc8bf6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     def __init__(self, **kwargs):\n",
    "\n",
    "#         self.filter_by = kwargs.get('filter_by','campaign_id')\n",
    "#         self.filters = kwargs.get('filters','fdbieo5')\n",
    "#         self.start_date = kwargs.get('start_date','2022-01-10')\n",
    "#         self.end_date = kwargs.get('end_date', '2022-01-11')\n",
    "    \n",
    "    def getPathFromAthena(self):\n",
    "        path = AthenaConnection()\n",
    "        print(f'ATHENA FILE PATH == {path}')\n",
    "\n",
    "        return path\n",
    "\n",
    "    def getData(self, file_path=None, file_type=None):\n",
    "        if file_path is None:\n",
    "            c = AthenaConnection()\n",
    "            print(c)\n",
    "            c.getS3Path()\n",
    "            print(f'ATHENA FILE PATH == {file_path}')\n",
    "        if file_type is None:\n",
    "            file_type = 'csv'\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def createSparkConnection(self):\n",
    "        spark= SparkConnection()\n",
    "\n",
    "        return spark\n",
    "\n",
    "    def loadFile(self, df, file_path, file_type, partitions):\n",
    "        df.write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .partitionBy(partitions)\\\n",
    "        .file_type(file_path)\n",
    "\n",
    "    def score_data(self, *kwargs):\n",
    "\n",
    "        # get input data for scoring\n",
    "        # self.getPathFromAthena()\n",
    "        \n",
    "        file_path  = self.getData()\n",
    "\n",
    "        # create connection\n",
    "        self.spark = createSparkConnection()\n",
    "        # self.sc = self.spark.sparkContext\n",
    "\n",
    "        print(f'SPARK SESSION =  {self.spark}')\n",
    "        # print(F'SPARK CONTEXT = {self.sc}')\n",
    "\n",
    "        df = self.spark.read.format('csv')\\\n",
    "        .options(header='true', inferSchema='true')\\\n",
    "        .load(file_path)\n",
    "\n",
    "\n",
    "        # score data\n",
    "        score = Score_T(kpi='ER',\n",
    "                        modelParams=self.model_params,\n",
    "                        verbose=2,\n",
    "                        mutualInformation=False)\n",
    "\n",
    "        df_scored = self.df.transform(score.transform)\n",
    "\n",
    "        # save scored file\n",
    "        self.loadFile(df_scored)\n",
    "        score = Score_T.transform(self, self.df)\n",
    "        \n",
    "        print(score)\n",
    "\n",
    "        # print(df_scored)\n",
    "        \n",
    "        return df_scored\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     c = SparkPipeline()\n",
    "#     c.score_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03ffa9-34de-4f9c-bc8e-868a10527efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb357eb7-b417-4a10-9349-d6fe75d80a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_path(**kwargs):\n",
    "    filter_by = kwargs.get('filter_by','AdvertiserId')\n",
    "    filters = kwargs.get('filters',None)\n",
    "    start_date = kwargs.get('start_date',None)\n",
    "    end_date = kwargs.get('end_date', None)\n",
    "    \n",
    "    path = fetch.fetch_for_purpose(dry_run = True,\n",
    "                                   **{'purpose':'cpe', 'date':[start_date, end_date], 'filter':{'filters':'filter_by'}})\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca92fa-2e1e-4630-a915-a7da40752d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = get_s3_path(filter_by='AdvertiserId',\n",
    "                      filters = 'qo2pgri',\n",
    "                      start_date='2020-06-09',\n",
    "                      end_date='2022-03-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd8601-103a-4e75-a91b-68d0896291e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = self.spark.read.format('csv')\\\n",
    "        .options(header='true', inferSchema='true')\\\n",
    "        .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d23542-2e87-4556-871a-84f52e193c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7664e-bc1a-4542-83cf-f9c7b0153c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:algos]",
   "language": "python",
   "name": "conda-env-algos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
