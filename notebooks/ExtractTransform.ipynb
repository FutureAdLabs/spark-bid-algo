{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3877cf8e-a93a-4b65-ae0e-bb66f845ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "# importing libraries\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from kmodes.kmodes import KModes\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import pyludio.rds as dbapi\n",
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# import fuzzy_pandas as fpd\n",
    "from collections import OrderedDict\n",
    "# !pip install matplotlib\n",
    "# import matplotlibdroppyplot as plt\n",
    "\n",
    "# from polyfuzz import PolyFuzz\n",
    "# from polyfuzz.models import Embeddings, TFIDF, RapidFuzz\n",
    "# from flair.embeddings import WordEmbeddings\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "# Had it not been a root warning, one could use the following snippet instead.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import pyludio.ttd_api as tapi\n",
    "\n",
    "import argparse\n",
    "from email.policy import default\n",
    "import os, sys\n",
    "import gc as garbageCollector\n",
    "import shutil\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "# import pyludio.adutils as adu\n",
    "\n",
    "\n",
    "# from helpers.mutual_info import MutulInformation as MIFO\n",
    "# from bidding_algo import BiddingAlgo\n",
    "# from utils.algosbot import AlgoBot\n",
    "# from utils.rds import get_row_by_key\n",
    "# from utils.mlflow import scores_are_different\n",
    "# import ttd.algos_common as ac\n",
    "# from fpflow import fpflow\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "import pyspark.sql as pys\n",
    "import pyspark.sql.functions as psf\n",
    "import pyspark.ml as Pipeline\n",
    "import pyspark.ml.pipeline as pmpip \n",
    "import pyspark.ml.param as pmparam \n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7703212-1df6-4564-896c-2d59a928768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a031dd1-f1a4-4411-8d9c-ab0f40590303",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d7f262-865f-41a5-80a0-b2ff49969a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f492aa07b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setAppName('Large scale scoring2')\n",
    "conf.set('spark.executor.memory', '10g'),\n",
    "conf.set('spark.executor.cores', '4'),\n",
    "conf.set('spark.cores.max', '4'),\n",
    "conf.set('spark.driver.memory','10g'),\n",
    "conf.set('spark.master', 'yarn')\n",
    "conf.set('spark.sql.shuffle.partitions', 5000)\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "conf.set('spark.ui.port', '4044')\n",
    "conf.set('spark.driver.port', '8887')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab54899-9e2c-4648-8dd3-d066bec0cc10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/maven-slf4j-provider-3.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/05 16:53:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eed09f4-3ff6-4e92-823b-66a6153b5d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216b824f-e1ac-4626-8739-88f9809eba26",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.dynamicAllocation.enabled', 'false'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem',\n",
       "  '2'),\n",
       " ('spark.yarn.executor.memoryOverheadFactor', '0.1875'),\n",
       " ('spark.sql.parquet.output.committer.class',\n",
       "  'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'),\n",
       " ('spark.driver.port', '8887'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'ip-10-3-0-36.eu-west-1.compute.internal'),\n",
       " ('spark.blacklist.decommissioning.timeout', '1h'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1646230662462_0178'),\n",
       " ('spark.app.id', 'application_1646230662462_0178'),\n",
       " ('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.sql.emr.internal.extensions',\n",
       "  'com.amazonaws.emr.spark.EmrSparkSessionExtensions'),\n",
       " ('spark.executor.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.driver.host', 'ip-10-3-0-36.eu-west-1.compute.internal'),\n",
       " ('spark.eventLog.dir', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.executor.defaultJavaOptions',\n",
       "  \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70\"),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://ip-10-3-0-36.eu-west-1.compute.internal:4044'),\n",
       " ('spark.sql.hive.metastore.sharedPrefixes',\n",
       "  'com.amazonaws.services.dynamodbv2'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs:///user/spark/warehouse'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://ip-10-3-0-36.eu-west-1.compute.internal:20888/proxy/application_1646230662462_0178'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'),\n",
       " ('spark.sql.shuffle.partitions', '5000'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem',\n",
       "  'true'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.executor.memory', '10g'),\n",
       " ('spark.hadoop.yarn.timeline-service.enabled', 'false'),\n",
       " ('spark.driver.memory', '10g'),\n",
       " ('spark.resourceManager.cleanupExpiredHost', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.yarn.dist.files', 'file:/etc/spark/conf/hive-site.xml'),\n",
       " ('spark.app.name', 'Large scale scoring2'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n",
       " ('spark.yarn.historyServer.address',\n",
       "  'ip-10-3-0-36.eu-west-1.compute.internal:18080'),\n",
       " ('spark.driver.defaultJavaOptions',\n",
       "  \"-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\"),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '4044'),\n",
       " ('spark.decommissioning.timeout.threshold', '20'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'),\n",
       " ('spark.blacklist.decommissioning.enabled', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e1eba2-9d92-4c95-bcfd-ee7b4336e6f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of the program is-  32.32932162284851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# ads = ['868ko1s', 'ba9rdlp', 'w46ix36', 'oeyv9tu']\n",
    "# files = [f\"s3://ml-box-data/log_and_track/{ad}/*/adludio_tracking.csv\" for ad in ads]\n",
    "# files = \"s3://ml-box-data/emr-notebooks/dibora/experimental\\ notebooks/sites_data.csv\"\n",
    "# files = \"s3://aws-athena-query-results-489880714178-eu-west-1/algo_report_data_location/*\"\n",
    "files = \"s3://ml-box-data/temp/*/data.csv\"\n",
    "# files = \"s3://ml-box-data/temp/2021-02-27/history.csv\"\n",
    "files = \"s3://aws-athena-query-results-489880714178-eu-west-1/cpe_sample_location/b2245587-3b1a-4ae5-a4e7-0b54a9023cdb.csv\"\n",
    "df = (spark.read.format('csv')\n",
    "      .options(header='true', inferSchema='true')\n",
    "      .load(files)\n",
    "        )\n",
    "# end time\n",
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(\"Execution time of the program is- \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e9815f-9fd6-4554-96b6-aa7dd2323585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[type: string, campaign_id: string, auction_id: string, line_item_id: string, server_ts: timestamp, CreativeIsTrackable: string, CreativeWasViewable: string, VideoEventComplete: string, PostImpressionEventType: string, AudienceID: string, CreativeId: string, AdFormat: string, Frequency: string, Site: string, ReferrerCategories: string, MatchedFoldPosition: string, UserHourOfWeek: string, CountryLong: string, Region: string, Metro: string, City: string, DeviceType: string, OSFamily: string, OS: string, Browser: string, Recency: string, PartnerCostInUSDollars: string, DeviceMake: string, DeviceModel: string, RenderingContext: string, PartnerCurrencyExchangeRateFromUSD: string, AdvertiserCurrencyExchangeRateFromUSD: string, AuctionType: string, ReferrerUrl: string, RedirectUrl: string, ChannelId: string, DisplayImpressionId: string, Keyword: string, KeywordId: string, MatchType: string, DistributionNetwork: string, RawUrl: string, ProcessedTime: string, AdvertiserId: string, LogEntryTime: string, AdGroupId: string, AdvertiserCurrency: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34c146a-a770-4624-bbf9-64f64f0ee538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00386005, 0.40652567, 0.20501093, ..., 0.10409807, 0.73423076,\n",
       "       0.02790922])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from numpy.random import rand\n",
    "\n",
    "eng = np.random.choice([0, 1], size=(314559338,), p=[1./3, 2./3])\n",
    "click = eng\n",
    "AdvertiserCurrency = rand(314559338)\n",
    "AdvertiserCurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40156a56-c18b-49df-a8d6-627030fa108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"engagement\", \n",
    "                   udf(lambda id: eng[id])(monotonically_increasing_id()))\n",
    "df = df.withColumn(\"click\", \n",
    "                   udf(lambda id: click[id])(monotonically_increasing_id()))\n",
    "df = df.withColumn(\"AdvertiserCurrency\", \n",
    "                   udf(lambda id: click[id])(monotonically_increasing_id()))\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0788fbb8-cbbb-49f2-844e-ed6841485f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LogEntryTime: string, ImpressionId: string, AdvertiserId: string, CampaignId: string, AdGroupId: string, AudienceID: string, AdFormat: string, Frequency: string, Site: string, ReferrerCategoriesList: string, FoldPosition: string, UserHourOfWeek: string, Country: string, Region: string, Metro: string, City: string, DeviceType: string, OSFamily: string, OS: string, Browser: string, Recency: string, PartnerCostInUSD: string, DeviceMake: string, DeviceModel: string, TDID: string, RenderingContext: string, PartnerCurrencyExchangeRateFromUSD: string, AdvertiserCurrencyExchangeRateFromUSD: string, AuctionType: string, engagement: string, click: string, AdvertiserCurrency: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c38621e-8f12-4393-a16e-305a543725f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newDF = df.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d12e00-7eae-4b97-a96b-d35f556ac6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef016e9-0601-4dc6-a271-feeb1701ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rahel/algos-cpe/\n",
      "***\tCredentials Already Exist\t***\n",
      "***\tCredentials Already Exist\t***\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys, os\n",
    "\n",
    "# sys.path.append(os.path.join(os.path.dirname('spark_model'), '..'))\n",
    "#import cpe\n",
    "cpe_path = f\"{os.environ['HOME']}/algos-cpe/\"\n",
    "# !cd $cpe_path ; git pull \n",
    "#\n",
    "print(cpe_path)\n",
    "if not cpe_path in sys.path:\n",
    "    sys.path.append(cpe_path)\n",
    "\n",
    "# import pyludio.adutils as adu\n",
    "\n",
    "# from helpers.mutual_info import MutulInformation as MIFO\n",
    "from bidding_algo import BiddingAlgo\n",
    "from utils.algosbot import AlgoBot\n",
    "from utils.rds import get_row_by_key\n",
    "from utils.mlflow import scores_are_different\n",
    "import ttd.algos_common as ac\n",
    "from fpflow import fpflow\n",
    "from spark_components.transformers.score_transformer import Score_T\n",
    "from spark_components.transformers.parameters.score_parameters import *\n",
    "\n",
    "\n",
    "from spark_model.score import adlogdata\n",
    "from spark_model.config import adconfig\n",
    "from spark_model.bid import admodels\n",
    "# from spark_model.bid2 import admodels\n",
    "# from spark_model.bid3 import admodels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f32c9d4-b4be-4aa2-a412-2312cf86ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'AdvertiserId': 'toyw1ri',\n",
    " 'CampaignId': '5ih9e40',\n",
    " 'AlgoCampaignId': 'dn77n1o',\n",
    " 'CamapignName': 'NVPC',\n",
    " 'Owner': 'Advertiser',\n",
    " 'OwnerId': 'AdvertiserId',\n",
    " 'DataKey': 'AdvertiserId',\n",
    " 'ColumnFilters': ['AdvertiserId'],\n",
    " 'date_start': '2021-11-25',\n",
    " 'date_end': '2021-12-07',\n",
    " 'speed': 8,\n",
    " 'bfid': 4,\n",
    " 'basecpmbid': 1.0,\n",
    " 'mincpmbid': 1.0,\n",
    " 'maxcpmbid': 10.0,\n",
    " 'BidListId': 148759117,\n",
    " 'BlockListId': '',\n",
    " 'TargetListId': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd61ef4-cd38-4ffd-af15-05cbd8527490",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'AdvertiserId': 'toyw1ri', 'CampaignId': '5ih9e40', 'AlgoCampaignId': 'dn77n1o', 'CamapignName': 'NVPC', 'Owner': 'Advertiser', 'OwnerId': 'AdvertiserId', 'DataKey': 'AdvertiserId', 'ColumnFilters': ['AdvertiserId'], 'date_start': '2021-11-25', 'date_end': '2021-12-07', 'speed': 8, 'bfid': 4, 'basecpmbid': 1.0, 'mincpmbid': 1.0, 'maxcpmbid': 10.0, 'BidListId': 148759117, 'BlockListId': '', 'TargetListId': ''} 2\n",
      "---------- Getting training dimensions ----------\n",
      "---------- Training dimensions to be used are ----------\n",
      "['AdFormat', 'OS', 'FoldPosition', 'Site']\n",
      "---------- Getting fp target features ----------\n",
      "---------- Target features to be used are ----------\n",
      "['engagement', 'click', 'viewable', 'trackable', 'video-start', 'video-end', 'converted', 'AdvertiserCurrency', 'impression']\n",
      "---------- Training + Target features to be used are ----------\n",
      "['AdFormat', 'OS', 'FoldPosition', 'Site', 'engagement', 'click', 'viewable', 'trackable', 'video-start', 'video-end', 'converted', 'AdvertiserCurrency', 'impression']\n",
      "---------- KPIs being used are: ----------\n",
      "{'mutualInformation': False, 'scoringFunction': None, 'clean': False, 'boxPrice': False, 'costAggregation': {'AdvertiserCurrency': 'min'}, 'featureAggregation': {'target': 'sum', 'group_size': 'sum', 'trackable': 'sum', 'Recency': 'mean', 'Frequency': 'mean', 'LogEntryTime': 'nunique'}, 'kpiAggregation': {'engagement': 'sum', 'click': 'sum', 'vtr': 'sum', 'video-end': 'sum', 'video-start': 'sum', 'viewable': 'sum', 'impression': 'sum'}, 'rSample': False, 'kpi': ['ER', 'CTR'], 'beta': False, 'minGroupSize': 0, 'minScore': 0.0, 'verbose': 2, 'modelParams': {'AdvertiserId': 'toyw1ri', 'CampaignId': '5ih9e40', 'AlgoCampaignId': 'dn77n1o', 'CamapignName': 'NVPC', 'Owner': 'Advertiser', 'OwnerId': 'AdvertiserId', 'DataKey': 'AdvertiserId', 'ColumnFilters': ['AdvertiserId'], 'date_start': '2021-11-25', 'date_end': '2021-12-07', 'speed': 8, 'bfid': 4, 'basecpmbid': 1.0, 'mincpmbid': 1.0, 'maxcpmbid': 10.0, 'BidListId': 148759117, 'BlockListId': '', 'TargetListId': ''}}\n",
      "---------- Scoring function used is: FP ----------\n",
      "---------- Getting training dimensions ----------\n",
      "---------- Getting fp target features ----------\n",
      "---------- Using the following features: ----------\n",
      "['AdFormat', 'OS', 'FoldPosition', 'Site', 'engagement', 'click', 'AdvertiserCurrency']\n",
      "---------- Generating data type casting dictionary ----------\n",
      "---------- WARNNING: Dropping missing rows with any missing values ----------\n",
      "root\n",
      " |-- AdFormat: string (nullable = true)\n",
      " |-- OS: string (nullable = true)\n",
      " |-- FoldPosition: string (nullable = true)\n",
      " |-- Site: string (nullable = true)\n",
      " |-- engagement: string (nullable = true)\n",
      " |-- click: string (nullable = true)\n",
      " |-- AdvertiserCurrency: string (nullable = true)\n",
      "\n",
      "None\n",
      "---------- Getting training dimensions ----------\n",
      "---------- Hashing features using ['AdFormat', 'OS', 'FoldPosition', 'Site'] ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Data with shape 314559337  will be aggregated as:  ----------\n",
      "{'AdFormat': ('AdFormat', 'first'), 'OS': ('OS', 'first'), 'FoldPosition': ('FoldPosition', 'first'), 'Site': ('Site', 'first'), 'engagement': ('engagement', 'sum'), 'click': ('click', 'sum'), 'AdvertiserCurrency': ('AdvertiserCurrency', 'min'), 'group_size': ('group_size', 'sum')}\n",
      "---------- Computing for KPI rates ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Aggregated data shape is 3572001 ----------\n",
      "---------- Unique values of feature are: ----------\n",
      "----------\n",
      "---------- Scoring ER ----------\n",
      "---------- Scoring CTR ----------\n",
      "---------- Filtering transformed data set using minscore = 0.0 and mingroupsize 0  ----------\n",
      "Execution time of the program is-  1423.7556285858154\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "score = Score_T(kpi=['ER','CTR'],modelParams=model_params ,verbose=2,mutualInformation=False)\n",
    "df_scored = df.transform(score.transform)\n",
    "\n",
    "# end time\n",
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(\"Execution time of the program is- \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0678c4-ca21-403f-98ec-2d85585d3417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hash: string, Site: string, AdFormat: string, OS: string, FoldPosition: string, AdvertiserCurrency: double, group_size: bigint, engagement: double, click: double, ER: double, CTR: double, score_ER: double, score_CTR: double, score: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27055bde-7f90-45dd-bd5a-56dcb0625f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3572001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/04 13:42:20 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_10_python !\n",
      "22/04/04 13:42:20 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_11_python !\n",
      "22/04/04 13:42:20 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_9_python !\n"
     ]
    }
   ],
   "source": [
    "df_scored.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f60b945-bc40-468c-a0cc-4d6c620b4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/04 17:03:25 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_11_python !\n",
      "22/04/04 17:03:25 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_9_python !\n",
      "22/04/04 17:03:25 ERROR YarnScheduler: Lost executor 1 on ip-10-3-0-167.eu-west-1.compute.internal: Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:03:25 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:03:25 WARN TaskSetManager: Lost task 1.0 in stage 16.0 (TID 8681, ip-10-3-0-167.eu-west-1.compute.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:03:25 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 8680, ip-10-3-0-167.eu-west-1.compute.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:03:25 WARN TaskSetManager: Lost task 3.0 in stage 16.0 (TID 8683, ip-10-3-0-167.eu-west-1.compute.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:03:25 WARN TaskSetManager: Lost task 2.0 in stage 16.0 (TID 8682, ip-10-3-0-167.eu-west-1.compute.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1646230662462_0157_01_000002 on host: ip-10-3-0-167.eu-west-1.compute.internal. Exit status: 137. Diagnostics: [2022-04-04 17:03:25.390]Container killed on request. Exit code is 137\n",
      "[2022-04-04 17:03:25.396]Container exited with a non-zero exit code 137. \n",
      "[2022-04-04 17:03:25.397]Killed by external signal\n",
      ".\n",
      "22/04/04 17:04:25 WARN TaskSetManager: Lost task 1.1 in stage 16.0 (TID 8687, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n",
      "    command = serializer.loads(command.value)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n",
      "    self._value = self.load_from_path(self._path)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n",
      "    return self.load(f)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n",
      "    return pickle.load(file)\n",
      "MemoryError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "22/04/04 17:04:29 WARN TaskSetManager: Lost task 0.1 in stage 16.0 (TID 8686, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:83)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "22/04/04 17:04:48 WARN TaskSetManager: Lost task 3.2 in stage 16.0 (TID 8690, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n",
      "    command = serializer.loads(command.value)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n",
      "    self._value = self.load_from_path(self._path)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n",
      "    return self.load(f)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n",
      "    return pickle.load(file)\n",
      "MemoryError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "22/04/04 17:05:01 WARN TaskSetManager: Lost task 4.1 in stage 16.0 (TID 8695, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n",
      "    command = serializer.loads(command.value)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n",
      "    self._value = self.load_from_path(self._path)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n",
      "    return self.load(f)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n",
      "    return pickle.load(file)\n",
      "MemoryError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "22/04/04 17:05:01 ERROR TaskSetManager: Task 2 in stage 16.0 failed 4 times; aborting job\n",
      "22/04/04 17:05:01 ERROR FileFormatWriter: Aborting job 397061d6-8ae2-47e2-890c-8895aa40d82a.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 4 times, most recent failure: Lost task 2.3 in stage 16.0 (TID 8694, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n",
      "    command = serializer.loads(command.value)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n",
      "    self._value = self.load_from_path(self._path)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n",
      "    return self.load(f)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n",
      "    return pickle.load(file)\n",
      "MemoryError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:164)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:181)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n",
      "    command = serializer.loads(command.value)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n",
      "    self._value = self.load_from_path(self._path)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n",
      "    return self.load(f)\n",
      "  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n",
      "    return pickle.load(file)\n",
      "MemoryError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "22/04/04 17:05:02 WARN TaskSetManager: Lost task 5.1 in stage 16.0 (TID 8696, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): TaskKilled (Stage cancelled)\n",
      "22/04/04 17:05:02 WARN TaskSetManager: Lost task 4.2 in stage 16.0 (TID 8697, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o545.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:181)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 4 times, most recent failure: Lost task 2.3 in stage 16.0 (TID 8694, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n    command = serializer.loads(command.value)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n    self._value = self.load_from_path(self._path)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n    return self.load(f)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n    return pickle.load(file)\nMemoryError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:164)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:163)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n\t... 39 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n    command = serializer.loads(command.value)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n    self._value = self.load_from_path(self._path)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n    return self.load(f)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n    return pickle.load(file)\nMemoryError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2433/816223150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_scored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://ml-box-data/scoreTransformed/scored_sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# end time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o545.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:181)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 4 times, most recent failure: Lost task 2.3 in stage 16.0 (TID 8694, ip-10-3-0-167.eu-west-1.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n    command = serializer.loads(command.value)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n    self._value = self.load_from_path(self._path)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n    return self.load(f)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n    return pickle.load(file)\nMemoryError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:164)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:163)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n\t... 39 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/worker.py\", line 76, in read_command\n    command = serializer.loads(command.value)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 146, in value\n    self._value = self.load_from_path(self._path)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 123, in load_from_path\n    return self.load(f)\n  File \"/mnt1/yarn/usercache/rahel/appcache/application_1646230662462_0157/container_1646230662462_0157_01_000003/pyspark.zip/pyspark/broadcast.py\", line 129, in load\n    return pickle.load(file)\nMemoryError\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_scored.write\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".csv(\"s3://ml-box-data/scoreTransformed/scored_sample\")\n",
    "\n",
    "# end time\n",
    "end = time.time()\n",
    "print(\"Execution time of the program is- \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c422c869-e998-4fc0-b605-db431629aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b07f1d-bc25-4d87-8a46-d186e1f217b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
